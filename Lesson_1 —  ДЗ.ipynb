{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Домашнее задание <a class=\"anchor\" id=\"hw\"></a><center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X = np.array([[ 1,  1],\n",
    "              [ 1,  1],\n",
    "              [ 1,  2],\n",
    "              [ 1,  5],\n",
    "              [ 1,  3],\n",
    "              [ 1,  0],\n",
    "              [ 1,  5],\n",
    "              [ 1, 10],\n",
    "              [ 1,  1],\n",
    "              [ 1,  2]])\n",
    "y = [45, 55, 50, 55, 60, 35, 75, 80, 50, 60]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_mse(y, y_pred):\n",
    "    err = np.mean((y - y_pred)**2)\n",
    "    return err"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "huXrhXQsZTMt"
   },
   "source": [
    "1. Подберите скорость обучения (eta) и количество итераций"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 185
    },
    "colab_type": "code",
    "id": "IDB22MQKMYaJ",
    "outputId": "4c03219e-a57c-4583-f439-6699fd0619bb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of objects = 10        \n",
      "Learning rate = 0.1        \n",
      "Initial weights = [1.  0.5] \n",
      "\n",
      "Iteration #0: W_new = [11.8 38.2], MSE = 3047.75\n",
      "Iteration #10: W_new = [12651.73553914 69617.0969639 ], MSE = 18310954068.05\n",
      "Iteration #20: W_new = [ 7732434.81888021 42641607.37852186], MSE = 9128819654907568.0\n",
      "Iteration #30: W_new = [1.06344502e+09 5.86454589e+09], MSE = 2.327920364266843e+20\n",
      "Iteration #40: W_new = [3.00127077e+10 1.65510116e+11], MSE = 2.545133529815933e+23\n",
      "Iteration #50: W_new = [1.55345341e+11 8.56677968e+11], MSE = 9.572295620500074e+24\n",
      "Iteration #60: W_new = [1.27742291e+11 7.04456313e+11], MSE = 9.351480126475957e+24\n",
      "Iteration #70: W_new = [1.38141953e+10 7.61806995e+10], MSE = 1.640858952828383e+23\n",
      "Iteration #80: W_new = [1.51674189e+08 8.36432543e+08], MSE = 3.125533537874439e+19\n",
      "Iteration #90: W_new = [116395.49988139 641638.79864291], MSE = 31317286806394.04\n",
      "Iteration #100: W_new = [48.41966454 22.99883908], MSE = 53518.86\n",
      "Iteration #110: W_new = [44.9771605   3.82798314], MSE = 43.97\n"
     ]
    }
   ],
   "source": [
    "n = X.shape[0]\n",
    "\n",
    "#eta = 1e-2 \n",
    "#n_iter = 100\n",
    "\n",
    "# измененные значения скорости обучения (eta) и количества итераций при которых сходимость наступает быстрее\n",
    "eta = 1e-1\n",
    "n_iter = 120\n",
    "\n",
    "W = np.array([1, 0.5])\n",
    "print(f'Number of objects = {n} \\\n",
    "       \\nLearning rate = {eta} \\\n",
    "       \\nInitial weights = {W} \\n')\n",
    "\n",
    "for i in range(n_iter):\n",
    "    y_pred = np.dot(X, W)\n",
    "    err = calc_mse(y, y_pred)\n",
    "    for k in range(W.shape[0]):\n",
    "        W[k] -= eta * (1/n * 2 * X[:, k] @ (y_pred - y))\n",
    "    if i % 10 == 0:\n",
    "        eta /= 1.1\n",
    "        print(f'Iteration #{i}: W_new = {W}, MSE = {round(err, 2)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5Qu1o4JhZYwI"
   },
   "source": [
    "2*. В этом коде мы избавляемся от итераций по весам, но тут есть ошибка, исправьте ее"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of objects = 10        \n",
      "Learning rate = 0.01        \n",
      "Initial weights = [1.  0.5] \n",
      "\n",
      "Iteration #0: W_new = [2.08 4.27], MSE = 3047.75\n",
      "Iteration #10: W_new = [ 7.0011236 10.6169007], MSE = 738.65\n",
      "Iteration #20: W_new = [10.3486292  10.10603105], MSE = 622.03\n",
      "Iteration #30: W_new = [13.38789582  9.55618391], MSE = 525.24\n",
      "Iteration #40: W_new = [16.16088505  9.05336203], MSE = 444.66\n",
      "Iteration #50: W_new = [18.69110735  8.59454545], MSE = 377.58\n",
      "Iteration #60: W_new = [20.99981865  8.17589626], MSE = 321.72\n",
      "Iteration #70: W_new = [23.10641138  7.79389815], MSE = 275.22\n",
      "Iteration #80: W_new = [25.02858024  7.44534246], MSE = 236.5\n",
      "Iteration #90: W_new = [26.78247081  7.12730145], MSE = 204.27\n",
      "Iteration #100: W_new = [28.38281518  6.83710367], MSE = 177.43\n",
      "Iteration #110: W_new = [29.84305573  6.57231156], MSE = 155.08\n",
      "Iteration #120: W_new = [31.17545797  6.33070096], MSE = 136.48\n",
      "Iteration #130: W_new = [32.39121367  6.11024241], MSE = 120.99\n",
      "Iteration #140: W_new = [33.50053475  5.90908413], MSE = 108.09\n",
      "Iteration #150: W_new = [34.51273915  5.72553647], MSE = 97.36\n",
      "Iteration #160: W_new = [35.43632906  5.55805768], MSE = 88.42\n",
      "Iteration #170: W_new = [36.27906231  5.405241  ], MSE = 80.98\n",
      "Iteration #180: W_new = [37.0480176   5.26580281], MSE = 74.78\n",
      "Iteration #190: W_new = [37.74965389  5.13857189], MSE = 69.62\n",
      "Iteration #200: W_new = [38.38986469  5.02247953], MSE = 65.33\n",
      "Iteration #210: W_new = [38.97402756  4.9165506 ], MSE = 61.75\n",
      "Iteration #220: W_new = [39.50704928  4.81989533], MSE = 58.77\n",
      "Iteration #230: W_new = [39.99340705  4.73170185], MSE = 56.29\n",
      "Iteration #240: W_new = [40.43718613  4.65122936], MSE = 54.23\n",
      "Iteration #250: W_new = [40.84211409  4.57780191], MSE = 52.51\n",
      "Iteration #260: W_new = [41.21159221  4.51080275], MSE = 51.08\n",
      "Iteration #270: W_new = [41.54872398  4.4496691 ], MSE = 49.89\n",
      "Iteration #280: W_new = [41.8563412   4.39388747], MSE = 48.9\n",
      "Iteration #290: W_new = [42.13702774  4.34298929], MSE = 48.07\n",
      "Iteration #300: W_new = [42.39314129  4.29654705], MSE = 47.39\n",
      "Iteration #310: W_new = [42.6268331   4.25417064], MSE = 46.81\n",
      "Iteration #320: W_new = [42.84006612  4.21550412], MSE = 46.34\n",
      "Iteration #330: W_new = [43.03463143  4.1802227 ], MSE = 45.94\n",
      "Iteration #340: W_new = [43.21216332  4.14803003], MSE = 45.61\n",
      "Iteration #350: W_new = [43.37415299  4.1186557 ], MSE = 45.34\n",
      "Iteration #360: W_new = [43.5219611   4.09185298], MSE = 45.11\n",
      "Iteration #370: W_new = [43.6568292   4.06739673], MSE = 44.92\n",
      "Iteration #380: W_new = [43.77989013  4.04508153], MSE = 44.76\n",
      "Iteration #390: W_new = [43.89217756  4.02471993], MSE = 44.63\n",
      "Iteration #400: W_new = [43.99463466  4.00614091], MSE = 44.52\n",
      "Iteration #410: W_new = [44.08812206  3.98918842], MSE = 44.42\n",
      "Iteration #420: W_new = [44.173425    3.97372004], MSE = 44.35\n",
      "Iteration #430: W_new = [44.25126001  3.95960587], MSE = 44.28\n",
      "Iteration #440: W_new = [44.32228086  3.94672733], MSE = 44.23\n",
      "Iteration #450: W_new = [44.38708413  3.93497626], MSE = 44.19\n",
      "Iteration #460: W_new = [44.44621412  3.92425394], MSE = 44.15\n",
      "Iteration #470: W_new = [44.50016751  3.91447033], MSE = 44.12\n",
      "Iteration #480: W_new = [44.5493975   3.90554323], MSE = 44.1\n",
      "Iteration #490: W_new = [44.5943176   3.89739766], MSE = 44.07\n",
      "Iteration #500: W_new = [44.63530512  3.8899652 ], MSE = 44.06\n",
      "Iteration #510: W_new = [44.67270435  3.88318343], MSE = 44.04\n",
      "Iteration #520: W_new = [44.70682942  3.87699538], MSE = 44.03\n",
      "Iteration #530: W_new = [44.73796697  3.87134906], MSE = 44.02\n",
      "Iteration #540: W_new = [44.76637856  3.86619706], MSE = 44.01\n",
      "Iteration #550: W_new = [44.79230282  3.86149609], MSE = 44.0\n",
      "Iteration #560: W_new = [44.81595752  3.85720668], MSE = 44.0\n",
      "Iteration #570: W_new = [44.83754134  3.85329279], MSE = 43.99\n",
      "Iteration #580: W_new = [44.85723558  3.84972154], MSE = 43.99\n",
      "Iteration #590: W_new = [44.87520567  3.84646294], MSE = 43.99\n",
      "Iteration #600: W_new = [44.89160255  3.84348962], MSE = 43.98\n",
      "Iteration #610: W_new = [44.90656394  3.8407766 ], MSE = 43.98\n",
      "Iteration #620: W_new = [44.92021553  3.83830109], MSE = 43.98\n",
      "Iteration #630: W_new = [44.93267197  3.83604231], MSE = 43.98\n",
      "Iteration #640: W_new = [44.94403791  3.83398127], MSE = 43.98\n",
      "Iteration #650: W_new = [44.95440879  3.83210067], MSE = 43.97\n",
      "Iteration #660: W_new = [44.96387175  3.83038471], MSE = 43.97\n",
      "Iteration #670: W_new = [44.97250627  3.82881898], MSE = 43.97\n",
      "Iteration #680: W_new = [44.98038486  3.82739031], MSE = 43.97\n",
      "Iteration #690: W_new = [44.98757372  3.82608673], MSE = 43.97\n"
     ]
    }
   ],
   "source": [
    "n = X.shape[0]\n",
    "\n",
    "eta = 1e-2 \n",
    "n_iter = 700\n",
    "\n",
    "W = np.array([1, 0.5])\n",
    "print(f'Number of objects = {n} \\\n",
    "       \\nLearning rate = {eta} \\\n",
    "       \\nInitial weights = {W} \\n')\n",
    "\n",
    "for i in range(n_iter):\n",
    "    y_pred = np.dot(X, W)\n",
    "    err = calc_mse(y, y_pred)\n",
    "#     for k in range(W.shape[0]):\n",
    "#         W[k] -= eta * (1/n * 2 * X[:, k] @ (y_pred - y))\n",
    "    # ИЗМЕНЕНИЯ\n",
    "    # матрица признаков должна быть транспонированной\n",
    "    W -= eta * (1/n * 2 * np.dot(X.T, y_pred - y))\n",
    "    #\n",
    "    if i % 10 == 0:\n",
    "        print(f'Iteration #{i}: W_new = {W}, MSE = {round(err,2)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3*. Вместо того, чтобы задавать количество итераций, задайте другое условие останова алгоритма - когда веса перестают изменяться меньше определенного порога $\\epsilon$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of objects = 10        \n",
      "Learning rate = 0.056        \n",
      "Initial weights = [1.  0.5] \n",
      "\n",
      "21.96121235269128\n",
      "21.187551196243522\n",
      "20.44125330774959\n",
      "19.721343715483144\n",
      "19.02688265872082\n",
      "18.35696428534988\n",
      "17.71071539961606\n",
      "17.087294257959392\n",
      "16.485889410976796\n",
      "15.905718589637107\n",
      "15.346027633956693\n",
      "14.806089462422355\n",
      "14.285203080523033\n",
      "13.782692626823026\n",
      "13.297906455077197\n",
      "12.830216250953315\n",
      "12.379016181988204\n",
      "11.943722079462942\n",
      "11.523770650938449\n",
      "11.118618722245964\n",
      "10.727742507777808\n",
      "10.35063690797234\n",
      "9.986814832933296\n",
      "9.635806551167846\n",
      "9.29715906246986\n",
      "8.970435494015156\n",
      "8.655214518773887\n",
      "8.351089795382\n",
      "8.057669428648701\n",
      "7.774575449910488\n",
      "7.501443316474201\n",
      "7.237921429422325\n",
      "6.983670669082972\n",
      "6.738363947494962\n",
      "6.501685777225306\n",
      "6.273331855921937\n",
      "6.053008666009132\n",
      "5.840433088956493\n",
      "5.635332033574898\n",
      "5.437442077814307\n",
      "5.246509123558922\n",
      "5.062288063935006\n",
      "4.884542462665495\n",
      "4.713044245023738\n",
      "4.547573399956045\n",
      "4.38791769295937\n",
      "4.233872389316414\n",
      "4.085239987305795\n",
      "3.9418299610195504\n",
      "3.8034585124343443\n",
      "3.669948332396283\n",
      "3.5411283701921725\n",
      "3.4168336113925117\n",
      "3.2969048636634595\n",
      "3.1811885502564454\n",
      "3.0695365108951367\n",
      "2.9618058097900093\n",
      "2.857858550520925\n",
      "2.7575616975378794\n",
      "2.6607869040394063\n",
      "2.5674103459971502\n",
      "2.477312562103749\n",
      "2.390378299429466\n",
      "2.3064963645810077\n",
      "2.2255594801635836\n",
      "2.1474641463547135\n",
      "2.0721105074052937\n",
      "1.9994022228902997\n",
      "1.929246343538056\n",
      "1.861553191473254\n",
      "1.7962362447150106\n",
      "1.7332120257770807\n",
      "1.6723999942229177\n",
      "1.6137224430336796\n",
      "1.5571043986525015\n",
      "1.5024735245732868\n",
      "1.4497600283471297\n",
      "1.3988965718840758\n",
      "1.34981818493239\n",
      "1.3024621816217796\n",
      "1.256768079961134\n",
      "1.2126775241853363\n",
      "1.1701342098495027\n",
      "1.129083811572698\n",
      "1.0894739133366906\n",
      "1.0512539412487676\n",
      "1.0143750986808744\n",
      "0.9787903037005189\n",
      "0.9444541287119499\n",
      "0.9113227422290036\n",
      "0.8793538527038953\n",
      "0.8485066543389193\n",
      "0.8187417748106401\n",
      "0.7900212248387195\n",
      "0.7623083495339249\n",
      "0.7355677814622228\n",
      "0.7097653953641048\n",
      "0.684868264470512\n",
      "0.6608446183587684\n",
      "0.6376638022939984\n",
      "0.6152962380034275\n",
      "0.5937133858328514\n",
      "0.5728877082363665\n",
      "0.5527926345522064\n",
      "0.5334025270191943\n",
      "0.5146926479899508\n",
      "0.49663912829856205\n",
      "0.47921893674191235\n",
      "0.46240985063534246\n",
      "0.4461904274046808\n",
      "0.4305399771780526\n",
      "0.41543853634218436\n",
      "0.400866842029146\n",
      "0.3868063075007065\n",
      "0.37323899839862135\n",
      "0.36014760983032656\n",
      "0.34751544426056924\n",
      "0.33532639018053145\n",
      "0.32356490152707895\n",
      "0.3122159778256626\n",
      "0.3012651450313729\n",
      "0.29069843704356574\n",
      "0.2805023778702981\n",
      "0.27066396441970353\n",
      "0.2611706498962256\n",
      "0.2520103277803923\n",
      "0.24317131637159883\n",
      "0.23464234387406843\n",
      "0.2264125340068725\n",
      "0.21847139211956762\n",
      "0.21080879179563808\n",
      "0.20341496192660233\n",
      "0.1962804742402072\n",
      "0.1893962312667273\n",
      "0.18275345472797236\n",
      "0.17634367433412748\n",
      "0.17015871697407453\n",
      "0.16419069628537172\n",
      "0.15843200259052645\n",
      "0.15287529318668697\n",
      "0.1475134829763247\n",
      "0.14233973542691747\n",
      "0.1373474538480838\n",
      "0.13253027297498193\n",
      "0.12788205084723012\n",
      "0.12339686097295335\n",
      "0.11906898476793862\n",
      "0.11489290426022414\n",
      "0.11086329505081281\n",
      "0.10697501952148851\n",
      "0.10322312028106348\n",
      "0.09960281384168708\n",
      "0.09610948451712005\n",
      "0.09273867853518748\n",
      "0.08948609835688218\n",
      "0.0863475971948625\n",
      "0.08331917372433029\n",
      "0.08039696697955317\n",
      "0.07757725142949522\n",
      "0.07485643222626931\n",
      "0.07223104062033847\n",
      "0.06969772953662916\n",
      "0.06725326930588178\n",
      "0.06489454354579235\n",
      "0.06261854518671324\n",
      "0.06042237263679385\n",
      "0.05830322608170223\n",
      "0.0562584039141833\n",
      "0.054285299288902425\n",
      "0.052381396798176716\n",
      "0.05054426926433724\n",
      "0.048771574644658144\n",
      "0.047061053044870876\n",
      "0.045410523837466125\n",
      "0.04381788288109699\n",
      "0.042281099837548346\n",
      "0.04079821558284342\n",
      "0.03936733970917154\n",
      "0.037986648114465706\n",
      "0.03665438067654225\n",
      "0.035368839008840254\n",
      "0.03412838429490315\n",
      "0.032931435198822454\n",
      "0.03177646584899247\n",
      "0.03066200389260497\n",
      "0.02958662861837771\n",
      "0.028548969145158296\n",
      "0.027547702674067445\n",
      "0.026581552801952037\n",
      "0.025649287894013183\n",
      "0.024749719513514293\n",
      "0.02388170090657744\n",
      "0.02304412554013223\n",
      "0.022235925691153197\n",
      "0.021456071085385265\n",
      "0.020703567583817927\n",
      "0.019977455915238397\n",
      "0.01927681045324828\n",
      "0.018600738036171367\n",
      "0.01794837682836311\n",
      "0.017318895221462526\n",
      "0.016711490774184087\n",
      "0.01612538918929615\n",
      "0.015559843326485037\n",
      "0.015014132249847338\n",
      "0.014487560308792903\n",
      "0.013979456251189301\n",
      "0.013489172367604186\n",
      "0.013016083665571164\n",
      "0.012559587072816129\n",
      "0.012119100668437535\n",
      "0.01169406294105078\n",
      "0.01128393207294566\n",
      "0.01088818524936304\n",
      "0.010506317991993292\n",
      "0.010137843515858367\n",
      "0.009782292108745263\n",
      "Iteration #217: W_new = [45.06287438  3.81731428], MSE = 43.97\n"
     ]
    }
   ],
   "source": [
    "n = X.shape[0]\n",
    "\n",
    "eta = 56e-3 \n",
    "\n",
    "# критерий сходимости (разница весов, при которой алгоритм останавливается)\n",
    "epsilon = 1e-2\n",
    "\n",
    "# зададим начальную разницу весов большим числом\n",
    "weight_epsilon = np.inf\n",
    "W = np.array([1, 0.5])\n",
    "i = 0\n",
    "print(f'Number of objects = {n} \\\n",
    "       \\nLearning rate = {eta} \\\n",
    "       \\nInitial weights = {W} \\n')\n",
    "\n",
    "while weight_epsilon > epsilon: \n",
    "    W_back = W.copy()\n",
    "    y_pred = np.dot(X, W_back)\n",
    "    err = calc_mse(y, y_pred)   \n",
    "    i+=1 \n",
    "    W -= eta * (1/n * 2 * np.dot(X.T, y_pred - y))    \n",
    "    weight_epsilon = np.linalg.norm(W - W_back, ord=2)\n",
    "    \n",
    "    print(weight_epsilon)\n",
    "    \n",
    "print(f'Iteration #{i}: W_new = {W}, MSE = {round(err,2)}')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Lesson_1.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
